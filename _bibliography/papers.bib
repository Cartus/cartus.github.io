---
---

@article{Lu2024ProcessDrivenAI,
  title={Process-Driven Autoformalization in Lean 4},
  author={Jianqiao Lu and Zhengying Liu and Yingjia Wan and Yinya Huang and Haiming Wang and Zhicheng YANG and Jing Tang and Zhijiang Guo},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.01940},
  url={https://api.semanticscholar.org/CorpusID:270226883}
}

@inproceedings{Lu2024FormalAlignAA,
  title={FormalAlign: Automated Alignment Evaluation for Autoformalization},
  author={Jianqiao Lu and Yingjia Wan and Yinya Huang and Jing Xiong and Zhengying Liu and Zhijiang Guo},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:273346369}
}

@article{Dai2024MHPPET,
  title={MHPP: Exploring the Capabilities and Limitations of Language Models Beyond Basic Code Generation},
  author={Jianbo Dai and Jianqiao Lu and Yunlong Feng and Rongju Ruan and Ming Cheng and Haochen Tan and Zhijiang Guo},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.11430},
  url={https://api.semanticscholar.org/CorpusID:269922079}
}

@article{Liu2024CtrlAAR,
  title={CtrlA: Adaptive Retrieval-Augmented Generation via Probe-Guided Control},
  author={Huanshuo Liu and Hao Zhang and Zhijiang Guo and Kuicai Dong and Xiangyang Li and Yi Quan Lee and Cong Zhang and Yong Liu},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.18727},
  url={https://api.semanticscholar.org/CorpusID:270094735}
}

@inproceedings{Huang2024EffiCodeUC,
  title={Effi-Code: Unleashing Code Efficiency in Language Models},
  author={Dong Huang and Guangtao Zeng and Jianbo Dai and Meng Luo and Han Weng and Yuhao Qing and Heming Cui and Zhijiang Guo and Jie M. Zhang},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:273345361}
}

@article{Xiong2024UNCompUL,
  title={UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference},
  author={Jing Xiong and Jianghan Shen and Fanghua Ye and Chaofan Tao and Zhongwei Wan and Jianqiao Lu and Xun Wu and Chuanyang Zheng and Zhijiang Guo and Lingpeng Kong and Ngai Wong},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.03090},
  url={https://api.semanticscholar.org/CorpusID:273162765}
}

@article{Liu2024AligningWL,
  title={Aligning with Logic: Measuring, Evaluating and Improving Logical Consistency in Large Language Models},
  author={Yinhong Liu and Zhijiang Guo and Tianya Liang and Ehsan Shareghi and Ivan Vuli'c and Nigel Collier},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.02205},
  url={https://api.semanticscholar.org/CorpusID:273098381}
}

@article{Lu2024YODATP,
  title={YODA: Teacher-Student Progressive Learning for Language Models},
  author={Jianqiao Lu and Wanjun Zhong and Yufei Wang and Zhijiang Guo and Qi Zhu and Wenyong Huang and Yanlin Wang and Fei Mi and Baojun Wang and Yasheng Wang and Lifeng Shang and Xin Jiang and Qun Liu},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.15670},
  url={https://api.semanticscholar.org/CorpusID:267311664}
}

@article{Su2023AreLR,
  title={Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof Generation with Contrastive Stepwise Decoding},
  author={Ying Su and Xiaojin Fu and Mingwen Liu and Zhijiang Guo},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.06736},
  url={https://api.semanticscholar.org/CorpusID:265150088}
}

@article{Hu2023GiveMM,
  title={Give Me More Details: Improving Fact-Checking with Latent Retrieval},
  author={Xuming Hu and Zhijiang Guo and Guan-Huei Wu and Lijie Wen and Philip S. Yu},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.16128},
  url={https://api.semanticscholar.org/CorpusID:258888167}
}

@article{Song2022GRATISDL,
  title={GRATIS: Deep Learning Graph Representation with Task-specific Topology and Multi-dimensional Edge Features},
  author={Siyang Song and Yuxin Song and Cheng Luo and Zhiyuan Song and Selim Kuzucu and Xi Jia and Zhijiang Guo and Weicheng Xie and Linlin Shen and Hatice Gunes},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.12482},
  url={https://api.semanticscholar.org/CorpusID:253760998}
}


@inproceedings{Lu2024AutoPSVAP,
  title={AutoPSV: Automated Process-Supervised Verifier},
  author={Jianqiao Lu and Zhiyang Dou and Hongru Wang and Zeyu Cao and Jianbo Dai and Yingjia Wan and Yinya Huang and Zhijiang Guo},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:270063532}
}

@inproceedings{Zeng2024MRBenAM,
  abbr={NeurIPS},
  title={MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs},
  author={Zhongshen Zeng and Yinhong Liu and Yingjia Wan and Jingyao Li and Pengguang Chen and Jianbo Dai and Yuxuan Yao and Rongwu Xu and Zehan Qi and Wanru Zhao and Linling Shen and Jianqiao Lu and Haochen Tan and Yukang Chen and Hao Zhang and Zhan Shi and Bailin Wang and Zhijiang Guo† and Jiaya Jia†},
  year={2024},
  preview={MrBen.png},
  booktitle={NeurIPS},
  selected={true},
  annotation={† denotes Corresponding Author.},
  html={https://arxiv.org/pdf/2406.13975v1},
  code={https://github.com/dvlab-research/Mr-Ben},
  website={https://randolph-zeng.github.io/Mr-Ben.github.io/},
  abstract={Large language models (LLMs) have shown increasing capability in problem-solving and decision-making, largely based on the step-by-step chain-of-thought reasoning processes. However, it has been increasingly challenging to evaluate the reasoning capability of LLMs. Concretely, existing outcome-based benchmarks begin to saturate and become less sufficient to monitor the progress. To this end, we present a process-based benchmark Mr-Ben that demands a meta reasoning skill, where LMs are asked to locate and analyse potential errors in automatically generated reasoning steps. Mr-Ben is a comprehensive benchmark comprising 5,975 questions collected from human experts, covering various subjects such as physics, chemistry, logic, coding, and more. Through our designed metrics for assessing meta-reasoning on this benchmark, we identify interesting limitations and weaknesses of current LLMs (open-source and closed-source models). For example, open-source models are seemingly comparable to GPT-4 on outcome-based benchmarks, but they lag far behind on our benchmark, revealing the underlying reasoning capability gap between them. Our dataset and codes are available on https://randolph-zeng.github.io/Mr-Ben.github.io/.} 
}

@article{Tian2024HydraLoRAAA,
  title={HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning},
  author={Chunlin Tian and Zhanying Shi and Zhijiang Guo and Li Li and Chengzhong Xu},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.19245},
  url={https://api.semanticscholar.org/CorpusID:269457298}
}

@inproceedings{Huang2024EffiLearnerEE,
  title={EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization},
  author={Dong Huang and Jianbo Dai and Han Weng and Puzhen Wu and Yuhao Qing and Jie M.Zhang and Heming Cui and Zhijiang Guo},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:270045278}
}

@article{Xu2024KnowledgeCF,
  title={Knowledge Conflicts for LLMs: A Survey},
  author={Rongwu Xu and Zehan Qi and Zhijiang Guo and Cunxiang Wang and Hongru Wang and Yue Zhang and Wei Xu},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.08319},
  url={https://api.semanticscholar.org/CorpusID:268379757}
}

@article{Zhang2024DoWN,
  title={Do We Need Language-Specific Fact-Checking Models? The Case of Chinese},
  author={Caiqi Zhang and Zhijiang Guo and Andreas Vlachos},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.15498},
  url={https://api.semanticscholar.org/CorpusID:267311897}
}

@article{Yao2024LearningFC,
  title={Learning From Correctness Without Prompting Makes LLM Efficient Reasoner},
  author={Yuxuan Yao and Han Wu and Zhijiang Guo and Biyan Zhou and Jiahui Gao and Sichun Luo and Hanxu Hou and Xiaojin Fu and Linqi Song},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.19094},
  url={https://api.semanticscholar.org/CorpusID:268733095}
}

@inproceedings{DBLP:conf/acl/TanGSXLFLWSLS24,
  abbr={ACL},
  author       = {Haochen Tan* and
                  Zhijiang Guo* and
                  Zhan Shi and
                  Lu Xu and
                  Zhili Liu and
                  Yunlong Feng and
                  Xiaoguang Li and
                  Yasheng Wang and
                  Lifeng Shang and
                  Qun Liu and
                  Linqi Song†},
  annotation   = {* denotes Equal Contribution. † denotes Corresponding Author.},
  title        = {ProxyQA: An Alternative Framework for Evaluating Long-Form Text Generation
                  with Large Language Models},
  booktitle  = {ACL},
  year       = {2024},
  html       = {https://arxiv.org/pdf/2401.15042.pdf},
  code       = {https://github.com/Namco0816/ProxyQA},
  website    = {https://proxy-qa.com/},
  selected   = {true},
  preview = {ProxyQA.png},
  abstract = {Large Language Models (LLMs) have succeeded remarkably in understanding long-form contents. However, exploring their capability for generating long-form contents, such as reports and articles, has been relatively unexplored and inadequately assessed by existing benchmarks. The prevalent evaluation methods, which predominantly rely on crowdsourcing, are recognized for their labor-intensive nature and lack of efficiency, whereas automated metrics, such as the ROUGE score, demonstrate discordance with human judgment criteria. In this paper, we propose ProxyQA, an innovative framework dedicated to assessing long-text generation. ProxyQA comprises in-depth human-curated meta-questions spanning various domains, each accompanied by specific proxy-questions with pre-annotated answers. LLMs are tasked to generate extensive content in response to these meta-questions, by engaging an evaluator and incorporating the generated texts as contextual background, ProxyQA assesses the generated content{'}s quality through the evaluator{'}s accuracy in addressing the proxy-questions. We examine multiple LLMs, emphasizing ProxyQA{'}s demanding nature as a high-quality assessment tool. Human evaluation demonstrates that the proxy-question method is notably self-consistent and aligns closely with human evaluative standards. The dataset and leaderboard is available at \url{https://proxy-qa.com}.},
}

@article{Hu2024EvaluatingRO,
  title={Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions},
  author={Xuming Hu and Xiaochuan Li and Junzhe Chen and Yinghui Li and Yangning Li and Xiaoguang Li and Yasheng Wang and Qun Liu and Lijie Wen and Philip S. Yu and Zhijiang Guo},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.12077},
  url={https://api.semanticscholar.org/CorpusID:268531485}
}






@article{Schlichtkrull2023AVeriTeCAD,
  title={AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web},
  author={M. Schlichtkrull and Zhijiang Guo and Andreas Vlachos},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.13117},
  url={https://api.semanticscholar.org/CorpusID:258832336}
}




@article{Xiong2023TRIGOBF,
  title={TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models},
  author={Jing Xiong and Jianhao Shen and Ye Yuan and Haiming Wang and Yichun Yin and Zhengying Liu and Lin Li and Zhijiang Guo and Qingxing Cao and Yinya Huang and Chuanyang Zheng and Xiaodan Liang and Ming Zhang and Qun Liu},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.10180},
  url={https://api.semanticscholar.org/CorpusID:264146203}
}




@article{Hu2023DoLL,
  title={Do Large Language Models Know about Facts?},
  author={Xuming Hu and Junzhe Chen and Xiaochuan Li and Yufei Guo and Lijie Wen and Philip S. Yu and Zhijiang Guo},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.05177},
  url={https://api.semanticscholar.org/CorpusID:263828951}
}



@article{Hu2023MultimodalRE,
  title={Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis},
  author={Xuming Hu and Zhijiang Guo and Zhiyang Teng and Irwin King and Philip S. Yu},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.16166},
  url={https://api.semanticscholar.org/CorpusID:258887662}
}


@article{Hu2023MR2AB,
  title={MR2: A Benchmark for Multimodal Retrieval-Augmented Rumor Detection in Social Media},
  author={Xuming Hu and Zhijiang Guo and Junzhe Chen and Lijie Wen and Philip S. Yu},
  journal={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={2023},
  html       = {},
  code       = {},
  website    = {},
  selected   = {false},
  preview = {},
  abstract = {}
  annotation   = {* denotes Equal Contribution. † denotes Corresponding Author.},
}


@article{Hu2023ReadIT,
  title={Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence},
  author={Xuming Hu and Zhaochen Hong and Zhijiang Guo and Lijie Wen and Philip S. Yu},
  journal={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={2023},
  booktitle={SIGIR},
  html       = {},
  code       = {},
  website    = {},
  selected   = {false},
  preview = {},
  abstract = {}
  annotation   = {* denotes Equal Contribution. † denotes Corresponding Author.},
}


@article{Guo2022ASO,
  title={A Survey on Automated Fact-Checking},
  author={Zhijiang Guo and M. Schlichtkrull and Andreas Vlachos},
  journal={TACL},
  year={2022},
  html       = {},
  code       = {},
  website    = {},
  selected   = {false},
  preview = {},
  abstract = {}
  annotation   = {* denotes Equal Contribution. † denotes Corresponding Author.},
}


@inproceeding{Zhou2022METSCoVAD,
  title={METS-CoV: A Dataset of Medical Entity and Targeted Sentiment on COVID-19 Related Tweets},
  author={Peilin Zhou and Zeqiang Wang and Dading Chong and Zhijiang Guo and Yining Hua and Zichang Su and Zhiyang Teng and Jiageng Wu and Jie Yang},
  booktitle={NeurIPS},
  year={2022},
  html       = {},
  code       = {},
  website    = {},
  selected   = {false},
  preview = {},
  abstract = {}
  annotation   = {* denotes Equal Contribution. † denotes Corresponding Author.},
}

@inproceedings{Hu2022SceneGM,
  title={Scene Graph Modification as Incremental Structure Expanding},
  author={Xuming Hu and Zhijiang Guo and Yuwei Fu and Lijie Wen and Philip S. Yu},
  booktitle={COLING},
  year={2022},
  html       = {},
  code       = {},
  website    = {},
  selected   = {false},
  preview = {},
  abstract = {}
  annotation   = {* denotes Equal Contribution. † denotes Corresponding Author.},
}

@inproceedings{Hu2022CHEFAP,
  title={CHEF: A Pilot Chinese Dataset for Evidence-Based Fact-Checking},
  author={Xuming Hu and Zhijiang Guo and Guan-Huei Wu and Aiwei Liu and Lijie Wen and Philip S. Yu},
  year={2022},
  booktitle={EMNLP},
  html       = {},
  code       = {},
  website    = {},
  selected   = {false},
  preview = {},
  abstract = {}
  annotation   = {* denotes Equal Contribution. † denotes Corresponding Author.},
}

@inproceedings{Aly2021FEVEROUSFE,
  title={FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information},
  author={Rami Aly and Zhijiang Guo and M. Schlichtkrull and James Thorne and Andreas Vlachos and Christos Christodoulopoulos and Oana Cocarascu and Arpit Mittal},
  booktitle={NeurIPS},
  year={2021},
  html       = {},
  code       = {},
  website    = {},
  selected   = {false},
  preview = {},
  abstract = {}
  annotation   = {* denotes Equal Contribution. † denotes Corresponding Author.},
}

@inproceedings{Nan2021UncoveringMC,
  title={Uncovering Main Causalities for Long-tailed Information Extraction},
  author={Guoshun Nan and Jiaqi Zeng and Rui Qiao and Zhijiang Guo and Wei Lu},
  booktitle={EMNLP},
  year={2021},
  html       = {https://arxiv.org/pdf/2109.05213},
  code       = {https://github.com/HeyyyyyyG/CFIE},
  selected   = {false},
  preview = {CFIE.png},
  abstract = {Information Extraction (IE) aims to extract structural information from unstructured texts. In practice, long-tailed distributions caused by the selection bias of a dataset, may lead to incorrect correlations, also known as spurious correlations, between entities and labels in the conventional likelihood models. This motivates us to propose counterfactual IE (CFIE), a novel framework that aims to uncover the main causalities behind data in the view of causal inference. Specifically, 1) we first introduce a unified structural causal model (SCM) for various IE tasks, describing the relationships among variables; 2) with our SCM, we then generate counterfactuals based on an explicit language structure to better calculate the direct causal effect during the inference stage; 3) we further propose a novel debiasing approach to yield more robust predictions. Experiments on three IE tasks across five public datasets show the effectiveness of our CFIE model in mitigating the spurious correlation issues.}
}

@inproceedings{Zhang2020LightweightDG,
  title={Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text Generation},
  author={Yan Zhang* and Zhijiang Guo* and Zhiyang Teng and Wei Lu and Shay B. Cohen and Zuozhu Liu and Lidong Bing},
  booktitle={EMNLP},
  year={2020}
  html       = {https://arxiv.org/pdf/2010.04383},
  code       = {https://github.com/yanzhangnlp/LDGCNs},
  selected   = {false},
  preview = {LDGCN.png},
  abstract = {AMR-to-text generation is used to transduce Abstract Meaning Representation structures (AMR) into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks (GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local (first-order) information aggregation scheme. To account for these issues, larger and deeper GCN models are required to capture more complex interactions. In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local interactions by synthesizing higher order information from the input graphs. We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity. With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity. Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters.}
  annotation   = {* denotes Equal Contribution.},
}

@inproceedings{Guo2020LearningLF,
  title={Learning Latent Forests for Medical Relation Extraction},
  author={Zhijiang Guo* and Guoshun Nan* and Wei Lu and Shay B. Cohen},
  booktitle={IJCAI},
  year={2020},
  html       = {https://www.ijcai.org/proceedings/2020/0505.pdf},
  code       = {https://github.com/Cartus/Latent-Forests},
  selected   = {false},
  preview = {LFGCN.png},
  abstract = {The goal of medical relation extraction is to detect relations among entities, such as genes, mutations and drugs in medical texts. Dependency tree structures have been proven useful for this task. Existing approaches to such relation extraction leverage off-the-shelf dependency parsers to obtain a syntactic tree or forest for the text. However, for the medical domain, low parsing accuracy may lead to error propagation downstream the relation extraction pipeline. In this work, we propose a novel model which treats the dependency structure as a latent variable and induces it from the unstructured text in an end-to-end fashion. Our model can be understood as composing task-specific dependency forests that capture non-local interactions for better relation extraction. Extensive results on four datasets show that our model is able to significantly outperform state-of-the-art systems without relying on any direct tree supervision or pre-training.}
  annotation   = {* denotes Equal Contribution.},
}

@inproceedings{Nan2020ReasoningWL,
  title={Reasoning with Latent Structure Refinement for Document-Level Relation Extraction},
  author={Guoshun Nan* and Zhijiang Guo* and Ivan Sekulic and Wei Lu},
  booktitle={ACL},
  year={2020},
  html       = {https://arxiv.org/pdf/2005.06312},
  code       = {https://github.com/nanguoshun/LSR},
  selected   = {false},
  preview = {DyReasoner.png},
  annotation   = {* denotes Equal Contribution.},
  abstract = {Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.}
}


@inproceedings{Guo2019AttentionGG,
  title={Attention Guided Graph Convolutional Networks for Relation Extraction},
  author={Zhijiang Guo* and Yan Zhang* and Wei Lu},
  booktitle={ACL},
  year={2019},
  html       = {https://arxiv.org/pdf/1906.07510},
  code       = {https://github.com/Cartus/AGGCN},
  selected   = {false},
  preview = {AGGCN.png},
  annotation   = {* denotes Equal Contribution.},
  abstract = {Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model which directly takes full dependency trees as inputs. Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches.}
}


@article{Guo2019DenselyCG,
  title={Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning},
  author={Zhijiang Guo* and Yan Zhang* and Zhiyang Teng and Wei Lu},
  journal={TACL},
  year={2019},
  html       = {https://arxiv.org/pdf/1908.05957},
  code       = {https://github.com/Cartus/DCGCN},
  selected   = {false},
  preview = {DCGCN.png},
  annotation   = {* denotes Equal Contribution.},
  abstract = {We focus on graph-to-sequence learning, which can be framed as transducing graph structures to sequences for text generation. To capture structural information associated with graphs, we investigate the problem of encoding graphs using graph convolutional networks (GCNs). Unlike various existing approaches where shallow architectures were used for capturing local structural information only, we introduce a dense connection strategy, proposing a novel Densely Connected Graph Convolutional Networks (DCGCNs). Such a deep architecture is able to integrate both local and non-local features to learn a better structural representation of a graph. Our model outperforms the state-of-the-art neural models significantly on AMRto-text generation and syntax-based neural machine translation.}
}


@inproceedings{Guo2018BetterTA,
  title={Better Transition-Based AMR Parsing with a Refined Search Space},
  author={Zhijiang Guo and Wei Lu},
  year={2018},
  booktitle  = {EMNLP},
  html       = {https://aclanthology.org/D18-1198.pdf},
  code       = {https://github.com/Cartus/AMR-Parser},
  selected   = {false},
  preview = {AMRParser.png}
  abstract = {This paper introduces a simple yet effective transition-based system for Abstract Meaning Representation (AMR) parsing. We argue that a well-defined search space involved in a transition system is crucial for building an effective parser. We propose to conduct the search in a refined search space based on a new compact AMR graph and an improved oracle. Our end-to-end parser achieves the state-of-the-art performance on various datasets with minimal additional information.}
}

